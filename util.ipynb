{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def select_random_rows_from_npz(directory_path, k):\n",
    "    \"\"\"\n",
    "    Reads each .npz file in the given directory, randomly selects k rows from each file,\n",
    "    and returns the chosen rows.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing .npz files.\n",
    "        k (int): Number of rows to randomly select from each file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are filenames and values are arrays of selected rows.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    selected_rows = []\n",
    "\n",
    "    # Get a list of all .npz files in the directory\n",
    "    npz_files = [f for f in os.listdir(directory_path) if f.endswith('.npz')]\n",
    "\n",
    "    for filename in npz_files:\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Load the .npz file\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        # Retrieve all arrays stored in the .npz file\n",
    "        array_keys = data.files\n",
    "        # For this example, we'll use the first array\n",
    "        array = data[array_keys[0]]\n",
    "\n",
    "        num_rows = array.shape[0]\n",
    "\n",
    "        # Ensure that k does not exceed the number of rows\n",
    "        if k >= num_rows:\n",
    "            # If k is greater or equal to num_rows, select all rows\n",
    "            selected_indices = np.arange(num_rows)\n",
    "        else:\n",
    "            # Randomly select k unique indices without replacement\n",
    "            selected_indices = np.random.choice(num_rows, size=k, replace=False)\n",
    "\n",
    "        # Select the rows using the sampled indices\n",
    "        selected_data = array[selected_indices]\n",
    "\n",
    "        # Store the selected data in the dictionary with filename as the key\n",
    "        selected_rows.extend(selected_data)\n",
    "\n",
    "    # For large datasets and improved performance, consider using GPU acceleration with CuPy:\n",
    "    # import cupy as cp\n",
    "    # Replace numpy arrays with cupy arrays for GPU computations\n",
    "\n",
    "    return selected_rows\n",
    "\n",
    "out = select_random_rows_from_npz(\"/viscam/u/iamisaac/datacomp/small/metadata\", 2)\n",
    "print(out[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def select_random_rows(file_path, k, dataset_name='dataset'):\n",
    "    \"\"\"\n",
    "    Select k random rows from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the HDF5 file\n",
    "    k (int): Number of random rows to select\n",
    "    dataset_name (str): Name of the dataset in the HDF5 file (default: 'default_dataset')\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array containing k randomly selected rows\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get the dataset\n",
    "        dataset = f[dataset_name][\"url\"]\n",
    "\n",
    "        # Get the total number of rows\n",
    "        total_rows = dataset.shape[0]\n",
    "\n",
    "        # Ensure k is not larger than the total number of rows\n",
    "        k = min(k, total_rows)\n",
    "\n",
    "        # Generate k random indices\n",
    "        random_indices = np.random.choice(total_rows, k, replace=False)\n",
    "\n",
    "        # Select the random rows\n",
    "        # Start of Selection\n",
    "        # Sort the random indices as required by h5py for advanced indexing\n",
    "        sorted_indices = np.sort(random_indices)\n",
    "\n",
    "        # Retrieve the data corresponding to the sorted indices\n",
    "        random_rows_sorted = dataset[sorted_indices]\n",
    "        random_rows = []\n",
    "        for random_row in random_rows_sorted:\n",
    "            random_rows.append(random_row.decode('utf-8'))\n",
    "        \n",
    "    # Convert the random rows to a list and save to a JSON file\n",
    "    with open('random_urls.json', 'w') as json_file:\n",
    "        json.dump(random_rows, json_file)\n",
    "    return random_rows\n",
    "\n",
    "select_random_rows(\"/viscam/u/iamisaac/datacomp/small_merged/metadata.hdf5\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num failed = 271\n"
     ]
    }
   ],
   "source": [
    "from sfs_util import check_and_download_image\n",
    "import json\n",
    "\n",
    "download_path = \"/viscam/projects/sfs/mast3r/mast3r_outputs/random_imgs\"\n",
    "with open(\"random_urls.json\", \"r\") as file:\n",
    "    urls = json.load(file)\n",
    "num_failed = 0\n",
    "for url in urls:\n",
    "    try:\n",
    "        check_and_download_image(url, download_path, set())\n",
    "    except:\n",
    "        num_failed +=1\n",
    "print(f\"num failed = {num_failed}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_reconstruction_files(json_file_path, destination_dir):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, accumulates all file paths under \"reconstructionFile\",\n",
    "    and moves those files to the specified destination directory.\n",
    "\n",
    "    Parameters:\n",
    "        json_file_path (str): Path to the JSON file.\n",
    "        destination_dir (str): Path to the destination directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    def find_reconstruction_files(obj, collected_paths):\n",
    "        \"\"\"Recursively find all 'reconstructionFile' entries in the JSON object.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"reconstructionFile\":\n",
    "                    collected_paths.append(value)\n",
    "                else:\n",
    "                    find_reconstruction_files(value, collected_paths)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                find_reconstruction_files(item, collected_paths)\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Collect all paths\n",
    "    reconstruction_files = []\n",
    "    find_reconstruction_files(data, reconstruction_files)\n",
    "\n",
    "    # Move each file to the destination directory\n",
    "    for file_path in reconstruction_files:\n",
    "        if file_path.startswith(\"../\"):\n",
    "            file_path = file_path[3:]\n",
    "        if os.path.isfile(file_path):\n",
    "            # Get the base name of the file\n",
    "            file_name = os.path.basename(file_path)\n",
    "            dest_path = os.path.join(destination_dir, file_name)\n",
    "            \n",
    "            # Move the file\n",
    "            shutil.move(file_path, dest_path)\n",
    "            print(f\"Moved: {file_path} -> {dest_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "move_reconstruction_files(\"htmls/data_20241009_055216.json\", \"mast3r_outputs/random_high_quality_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unused_glb_files(json_file_paths, remove_dir):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, accumulates all file paths under \"reconstructionFile\",\n",
    "    and moves those files to the specified destination directory.\n",
    "\n",
    "    Parameters:\n",
    "        json_file_path (str): Path to the JSON file.\n",
    "        destination_dir (str): Path to the destination directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    def find_reconstruction_files(obj, collected_paths):\n",
    "        \"\"\"Recursively find all 'reconstructionFile' entries in the JSON object.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"reconstructionFile\":\n",
    "                    file_name = os.path.basename(value)\n",
    "                    collected_paths.add(file_name)\n",
    "                else:\n",
    "                    find_reconstruction_files(value, collected_paths)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                find_reconstruction_files(item, collected_paths)\n",
    "\n",
    "    # Collect all paths\n",
    "    reconstruction_files = set()\n",
    "    for json_file_path in json_file_paths:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        find_reconstruction_files(data, reconstruction_files)\n",
    "    \n",
    "    # Iterate through all .glb files under remove_dir\n",
    "    for root, dirs, files in os.walk(remove_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.glb'):\n",
    "                # If the file is not in the reconstruction_files set, delete it\n",
    "                if file not in reconstruction_files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "    \n",
    "    return reconstruction_files\n",
    "\n",
    "    # Move each file to the destination directory\n",
    "    # for file_path in reconstruction_files:\n",
    "    #     if file_path.startswith(\"../\"):\n",
    "    #         file_path = file_path[3:]\n",
    "    #     if os.path.isfile(file_path):\n",
    "    #         # Get the base name of the file\n",
    "    #         file_name = os.path.basename(file_path)\n",
    "    #         dest_path = os.path.join(destination_dir, file_name)\n",
    "            \n",
    "    #         # Move the file\n",
    "    #         shutil.move(file_path, dest_path)\n",
    "    #         print(f\"Moved: {file_path} -> {dest_path}\")\n",
    "    #     else:\n",
    "    #         print(f\"File not found: {file_path}\")\n",
    "delete_unused_glb_files([\"htmls/data_20241009_055216.json\", \"htmls/data_20241004_142239.json\"], \"mast3r_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
