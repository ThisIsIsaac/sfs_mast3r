{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "img1_path = \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7e253ff522632d5118acbecbc33591b9.jpg\"\n",
    "img2_path = \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7e253ff522632d5118acbecbc33591b9.jpg\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "dinov2_model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "\n",
    "img1 = Image.open(img1_path)\n",
    "img2 = Image.open(img2_path)\n",
    "mask1 = torch.tensor(mask1, dtype=torch.bool, device=device)\n",
    "mask2 = torch.tensor(mask2, dtype=torch.bool, device=device)\n",
    "height, width = mask1.shape\n",
    "\n",
    "\n",
    "inputs1 = processor(images=img1, return_tensors=\"pt\").to(device)\n",
    "out1 = dinov2_model(**inputs1)\n",
    "last_hidden1 = out1[0]\n",
    "\n",
    "inputs2 = processor(images=img2, return_tensors=\"pt\").to(device)\n",
    "last_hidden2 = dinov2_model(**inputs2)[0]\n",
    "\n",
    "# Remove the [CLS] token\n",
    "last_hidden1 = last_hidden1[:, 1:, :]  # Shape: [1, 256, 768]\n",
    "last_hidden2 = last_hidden2[:, 1:, :]  # Shape: [1, 256, 768]\n",
    "\n",
    "# Reshape to a square grid (assuming 14x14 patches)\n",
    "patch_size = 16\n",
    "feautre_dim = last_hidden1.shape[-1]\n",
    "reshaped_output1 = last_hidden1.reshape(1, patch_size, patch_size, feautre_dim)\n",
    "reshaped_output1 = reshaped_output1.permute(0, 3, 1, 2)\n",
    "reshaped_output1 = F.interpolate(reshaped_output1, size=(height, width), mode='bilinear')\n",
    "reshaped_output2 = last_hidden2.reshape(1, patch_size, patch_size, feautre_dim)\n",
    "reshaped_output2 = reshaped_output2.permute(0, 3, 1, 2)\n",
    "reshaped_output2 = F.interpolate(reshaped_output2, size=(height, width), mode='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def display_feature_maps(image_paths):\n",
    "    \"\"\"\n",
    "    Display a list of images alongside their feature norm maps computed using DINOv2.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list of str): List of file paths to the input images.\n",
    "    \"\"\"\n",
    "    # Define the device (use GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load the DINOv2 processor and model\n",
    "    processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "    dinov2_model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "\n",
    "    # Load and process all images\n",
    "    imgs = [Image.open(img_path).convert('RGB') for img_path in image_paths]\n",
    "    inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n",
    "    outputs = dinov2_model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_dim]\n",
    "\n",
    "    # Remove the [CLS] token from the outputs\n",
    "    last_hidden_states = last_hidden_states[:, 1:, :]  # Shape: [batch_size, num_patches, hidden_dim]\n",
    "\n",
    "    # Get the number of patches and determine the grid size\n",
    "    batch_size, num_patches, feature_dim = last_hidden_states.shape\n",
    "    patch_grid_size = int(num_patches ** 0.5)  # Assuming square grid\n",
    "\n",
    "    # Reshape the outputs to [batch_size, hidden_dim, height, width]\n",
    "    reshaped_outputs = last_hidden_states.reshape(batch_size, patch_grid_size, patch_grid_size, feature_dim)\n",
    "    reshaped_outputs = reshaped_outputs.permute(0, 3, 1, 2)  # Shape: [batch_size, hidden_dim, height, width]\n",
    "\n",
    "    # Set up the plot\n",
    "    num_images = len(image_paths)\n",
    "    plt.figure(figsize=(12, 6 * num_images))\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        img = imgs[idx]\n",
    "        reshaped_output = reshaped_outputs[idx:idx + 1]\n",
    "\n",
    "        # Upsample the feature map to match the original image size\n",
    "        height, width = img.size[1], img.size[0]\n",
    "        upsampled_output = F.interpolate(reshaped_output, size=(height, width), mode='bilinear')\n",
    "\n",
    "        # Compute the feature norm at each spatial location\n",
    "        feature_norm = upsampled_output.norm(dim=1)  # Shape: [1, height, width]\n",
    "        feature_norm = feature_norm.squeeze().detach().cpu().numpy()\n",
    "\n",
    "        # Normalize the feature norm for visualization\n",
    "        feature_norm -= feature_norm.min()\n",
    "        feature_norm /= feature_norm.max()\n",
    "\n",
    "        # Display the original image and the feature norm map side by side\n",
    "        plt.subplot(num_images, 2, 2 * idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Original Image {idx + 1}')\n",
    "\n",
    "        plt.subplot(num_images, 2, 2 * idx + 2)\n",
    "        plt.imshow(feature_norm, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Feature Norm Map {idx + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "display_feature_maps([\"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7dcb71ac3a054e86805a0aadbdd58f4d.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7cd086cc7da4a2f55ccff02868c47437.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7d10b9b39d3cf22d78fcd74c859b907e.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7d20e2c3ba8ee0b25e05702a767e985c.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7d37a071a0500ca58351240c8190abae.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7d71e81b5d039c7be0cd6d5d82e66098.jpg\",\n",
    "                      \"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/7dc00dac796e3329195508d8ca2b924d.jpg\",\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sfs_util\n",
    "from PIL import Image\n",
    "sfs_util.pil_to_tensor(\"/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/ffe43230ae908c5a39d003b733661f4c.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sfs\n",
    "from PIL import Image\n",
    "import torch\n",
    "mast3r = sfs.Mast3r(\n",
    "        num_responses_per_query=15,\n",
    "        min_conf_threshold = 1.5,\n",
    "        matching_conf_threshold = 2.0,\n",
    "    )\n",
    "with open(\"/viscam/projects/sfs/mast3r/clip_keywords.json\", 'r') as file:\n",
    "        clip_keywords = json.load(file)[\"bad\"]\n",
    "import os\n",
    "\n",
    "path = '/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/'\n",
    "# Use os.scandir for efficient directory traversal\n",
    "# os.scandir returns an iterator of os.DirEntry objects, which include the file path and type information\n",
    "# This is more efficient than os.listdir when dealing with large directories\n",
    "img_paths = [entry.path for entry in os.scandir(path) if entry.is_file() and entry.name.endswith('.jpg')][20:30]\n",
    "\n",
    "scores_dicts = mast3r.get_clip_scores2(img_paths, clip_keywords, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the images along with their corresponding dictionaries from scores_dicts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = [Image.open(img_path) for img_path in img_paths]\n",
    "\n",
    "fig, axes = plt.subplots(len(images), 2, figsize=(12, 6 * len(images)))\n",
    "\n",
    "for idx, (img, scores_dict) in enumerate(zip(images, scores_dicts)):\n",
    "    # Display image\n",
    "    axes[idx, 0].imshow(img)\n",
    "    axes[idx, 0].axis('off')\n",
    "    axes[idx, 0].set_title(f'Image {idx+1}')\n",
    "    \n",
    "    # Display scores dictionary\n",
    "    axes[idx, 1].axis('off')\n",
    "    # Format the scores_dict for display\n",
    "    scores_text = '\\n'.join([f'{k}: {v}' for k, v in scores_dict.items()])\n",
    "    axes[idx, 1].text(0.1, 0.5, scores_text, fontsize=12, va='center')\n",
    "    axes[idx, 1].set_title(f'Scores Dict for Image {idx+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sfs\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "mast3r = sfs.Mast3r(\n",
    "        num_responses_per_query=15,\n",
    "        min_conf_threshold = 1.5,\n",
    "        matching_conf_threshold = 2.0,\n",
    "    )\n",
    "with open(\"/viscam/projects/sfs/mast3r/clip_keywords.json\", 'r') as file:\n",
    "        clip_keywords = json.load(file)[\"classes\"]\n",
    "\n",
    "import os\n",
    "\n",
    "path = '/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/'\n",
    "all_img_paths = [entry.path for entry in os.scandir(path) if entry.is_file() and entry.name.endswith('.jpg')]\n",
    "\n",
    "for i in range(10):\n",
    "    imgs_per_iter = 5\n",
    "    img_paths = all_img_paths[imgs_per_iter*i:imgs_per_iter*(i+1)]\n",
    "    scores_dicts = mast3r.get_clip_scores(img_paths, clip_keywords, 10)\n",
    "\n",
    "\n",
    "    images = [Image.open(img_path) for img_path in img_paths]\n",
    "\n",
    "    fig, axes = plt.subplots(len(images), 2, figsize=(12, 6 * len(images)))\n",
    "\n",
    "    for idx, (img, scores_dict) in enumerate(zip(images, scores_dicts)):\n",
    "        # Display image\n",
    "        axes[idx, 0].imshow(img)\n",
    "        axes[idx, 0].axis('off')\n",
    "        axes[idx, 0].set_title(f'Image {idx+1}')\n",
    "        \n",
    "        # Display scores dictionary\n",
    "        axes[idx, 1].axis('off')\n",
    "        # Format the scores_dict for display\n",
    "        scores_text = '\\n'.join([f'{k}: {v}' for k, v in scores_dict.items()])\n",
    "        axes[idx, 1].text(0.1, 0.5, scores_text, fontsize=12, va='center')\n",
    "        axes[idx, 1].set_title(f'Scores Dict for Image {idx+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, CLIPModel, CLIPProcessor\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# Use os.scandir for efficient directory traversal\n",
    "# os.scandir returns an iterator of os.DirEntry objects, which include the file path and type information\n",
    "# This is more efficient than os.listdir when dealing with large directories\n",
    "with open(\"/viscam/projects/sfs/mast3r/clip_keywords.json\", 'r') as file:\n",
    "    json_file = json.load(file)\n",
    "    class_keywords = json_file[\"classes\"]\n",
    "    meta_class_keywords = json_file[\"meta_classes\"]\n",
    "    quality_keywords = json_file[\"quality\"]\n",
    "import os\n",
    "\n",
    "path = '/viscam/projects/sfs/mast3r/mast3r_outputs/imgs/'\n",
    "# Use os.scandir for efficient directory traversal\n",
    "# os.scandir returns an iterator of os.DirEntry objects, which include the file path and type information\n",
    "# This is more efficient than os.listdir when dealing with large directories\n",
    "all_img_paths = [entry.path for entry in os.scandir(path) if entry.is_file() and entry.name.endswith('.jpg')]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_dir = f'clip_test_imgs/clip_analysis_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "for i in range(20):\n",
    "    imgs_per_iter = 5\n",
    "    img_paths = all_img_paths[imgs_per_iter*i:imgs_per_iter*(i+1)]\n",
    "    imgs = [Image.open(image_path) for image_path in img_paths]\n",
    "\n",
    "    bottom_k_scores = []\n",
    "    for clip_keywords in [quality_keywords, meta_class_keywords, class_keywords]:\n",
    "        inputs = processor(text=clip_keywords, images=imgs, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "        imgs_probs = outputs.logits_per_image.softmax(dim=1).round(decimals=1).to(\"cpu\")\n",
    "        for img_path, probs in zip(img_paths, imgs_probs):\n",
    "            scores = {}\n",
    "            for prob, word in zip(probs, clip_keywords):\n",
    "                scores[word] = prob\n",
    "            sorted_pairs = sorted(scores.items(), reverse=True, key=lambda x: x[1])[:10]\n",
    "            lowest_k_dict = dict(sorted_pairs)\n",
    "            bottom_k_scores.append(lowest_k_dict)\n",
    "\n",
    "    fig, axes = plt.subplots(len(imgs), 2, figsize=(12, 6 * len(imgs)))\n",
    "\n",
    "    for idx, (img, scores_dict) in enumerate(zip(imgs, bottom_k_scores)):\n",
    "        # Display image\n",
    "        axes[idx, 0].imshow(img)\n",
    "        axes[idx, 0].axis('off')\n",
    "        axes[idx, 0].set_title(f'Image {idx+1}')\n",
    "\n",
    "        # Display scores dictionary\n",
    "        axes[idx, 1].axis('off')\n",
    "        # Format the scores_dict for display\n",
    "        scores_text = '\\n'.join([f'{k}: {v}' for k, v in scores_dict.items()])\n",
    "        axes[idx, 1].text(0.1, 0.5, scores_text, fontsize=12, va='center')\n",
    "        axes[idx, 1].set_title(f'Scores Dict for Image {idx+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to the timestamped directory\n",
    "    save_path = os.path.join(save_dir, f'plot_{i}.png')\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mast3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
